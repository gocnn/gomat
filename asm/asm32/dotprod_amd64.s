// Code generated by command: go run dotprod_asm.go -bits 32 -out ../../asm/asm32/dotprod_amd64.s -stubs ../../asm/asm32/dotprod_amd64.go -pkg asm32. DO NOT EDIT.

//go:build amd64 && gc && !noasm && !gccgo

#include "textflag.h"

// func DotProdAVX(x1 []float32, x2 []float32) float32
// Requires: AVX, FMA3, SSE
TEXT ·DotProdAVX(SB), NOSPLIT, $0-52
	MOVQ x1_base+0(FP), AX
	MOVQ x2_base+24(FP), CX
	MOVQ x1_len+8(FP), DX
	VZEROALL

unrolledLoop14:
	CMPQ        DX, $0x00000070
	JL          unrolledLoop8
	VMOVUPS     (AX), Y2
	VMOVUPS     32(AX), Y3
	VMOVUPS     64(AX), Y4
	VMOVUPS     96(AX), Y5
	VMOVUPS     128(AX), Y6
	VMOVUPS     160(AX), Y7
	VMOVUPS     192(AX), Y8
	VMOVUPS     224(AX), Y9
	VMOVUPS     256(AX), Y10
	VMOVUPS     288(AX), Y11
	VMOVUPS     320(AX), Y12
	VMOVUPS     352(AX), Y13
	VMOVUPS     384(AX), Y14
	VMOVUPS     416(AX), Y15
	VFMADD231PS (CX), Y2, Y0
	VFMADD231PS 32(CX), Y3, Y1
	VFMADD231PS 64(CX), Y4, Y0
	VFMADD231PS 96(CX), Y5, Y1
	VFMADD231PS 128(CX), Y6, Y0
	VFMADD231PS 160(CX), Y7, Y1
	VFMADD231PS 192(CX), Y8, Y0
	VFMADD231PS 224(CX), Y9, Y1
	VFMADD231PS 256(CX), Y10, Y0
	VFMADD231PS 288(CX), Y11, Y1
	VFMADD231PS 320(CX), Y12, Y0
	VFMADD231PS 352(CX), Y13, Y1
	VFMADD231PS 384(CX), Y14, Y0
	VFMADD231PS 416(CX), Y15, Y1
	ADDQ        $0x000001c0, AX
	ADDQ        $0x000001c0, CX
	SUBQ        $0x00000070, DX
	JMP         unrolledLoop14

unrolledLoop8:
	CMPQ        DX, $0x00000040
	JL          unrolledLoop4
	VMOVUPS     (AX), Y2
	VMOVUPS     32(AX), Y3
	VMOVUPS     64(AX), Y4
	VMOVUPS     96(AX), Y5
	VMOVUPS     128(AX), Y6
	VMOVUPS     160(AX), Y7
	VMOVUPS     192(AX), Y8
	VMOVUPS     224(AX), Y9
	VFMADD231PS (CX), Y2, Y0
	VFMADD231PS 32(CX), Y3, Y1
	VFMADD231PS 64(CX), Y4, Y0
	VFMADD231PS 96(CX), Y5, Y1
	VFMADD231PS 128(CX), Y6, Y0
	VFMADD231PS 160(CX), Y7, Y1
	VFMADD231PS 192(CX), Y8, Y0
	VFMADD231PS 224(CX), Y9, Y1
	ADDQ        $0x00000100, AX
	ADDQ        $0x00000100, CX
	SUBQ        $0x00000040, DX
	JMP         unrolledLoop8

unrolledLoop4:
	CMPQ        DX, $0x00000020
	JL          unrolledLoop1
	VMOVUPS     (AX), Y2
	VMOVUPS     32(AX), Y3
	VMOVUPS     64(AX), Y4
	VMOVUPS     96(AX), Y5
	VFMADD231PS (CX), Y2, Y0
	VFMADD231PS 32(CX), Y3, Y1
	VFMADD231PS 64(CX), Y4, Y0
	VFMADD231PS 96(CX), Y5, Y1
	ADDQ        $0x00000080, AX
	ADDQ        $0x00000080, CX
	SUBQ        $0x00000020, DX
	JMP         unrolledLoop4

unrolledLoop1:
	CMPQ        DX, $0x00000008
	JL          tail
	VMOVUPS     (AX), Y2
	VFMADD231PS (CX), Y2, Y0
	ADDQ        $0x00000020, AX
	ADDQ        $0x00000020, CX
	SUBQ        $0x00000008, DX
	JMP         unrolledLoop1

tail:
	VXORPS X2, X2, X2

tailLoop:
	CMPQ        DX, $0x00000000
	JE          reduce
	VMOVSS      (AX), X3
	VFMADD231SS (CX), X3, X2
	ADDQ        $0x00000004, AX
	ADDQ        $0x00000004, CX
	DECQ        DX
	JMP         tailLoop

reduce:
	VADDPS       Y0, Y1, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPS       X0, X1, X0
	VADDPS       X0, X2, X0
	VHADDPS      X0, X0, X0
	VHADDPS      X0, X0, X0
	MOVSS        X0, ret+48(FP)
	RET

// func DotProdSSE(x1 []float32, x2 []float32) float32
// Requires: SSE, SSE3
TEXT ·DotProdSSE(SB), NOSPLIT, $0-52
	MOVQ  x1_base+0(FP), AX
	MOVQ  x2_base+24(FP), CX
	MOVQ  x1_len+8(FP), DX
	XORPS X0, X0
	XORPS X1, X1
	CMPQ  DX, $0x00000000
	JE    reduce
	MOVQ  CX, BX
	ANDQ  $0x0000000f, BX
	JZ    unrolledLoops
	XORQ  $0x0000000f, BX
	INCQ  BX
	SHRQ  $0x02, BX

alignmentLoop:
	MOVSS (AX), X2
	MULSS (CX), X2
	ADDSS X2, X0
	ADDQ  $0x00000004, AX
	ADDQ  $0x00000004, CX
	DECQ  DX
	JZ    reduce
	DECQ  BX
	JNZ   alignmentLoop

unrolledLoops:
unrolledLoop14:
	CMPQ   DX, $0x00000038
	JL     unrolledLoop8
	MOVUPS (AX), X2
	MOVUPS 16(AX), X3
	MOVUPS 32(AX), X4
	MOVUPS 48(AX), X5
	MOVUPS 64(AX), X6
	MOVUPS 80(AX), X7
	MOVUPS 96(AX), X8
	MOVUPS 112(AX), X9
	MOVUPS 128(AX), X10
	MOVUPS 144(AX), X11
	MOVUPS 160(AX), X12
	MOVUPS 176(AX), X13
	MOVUPS 192(AX), X14
	MOVUPS 208(AX), X15
	MULPS  (CX), X2
	MULPS  16(CX), X3
	MULPS  32(CX), X4
	MULPS  48(CX), X5
	MULPS  64(CX), X6
	MULPS  80(CX), X7
	MULPS  96(CX), X8
	MULPS  112(CX), X9
	MULPS  128(CX), X10
	MULPS  144(CX), X11
	MULPS  160(CX), X12
	MULPS  176(CX), X13
	MULPS  192(CX), X14
	MULPS  208(CX), X15
	ADDPS  X2, X0
	ADDPS  X3, X1
	ADDPS  X4, X0
	ADDPS  X5, X1
	ADDPS  X6, X0
	ADDPS  X7, X1
	ADDPS  X8, X0
	ADDPS  X9, X1
	ADDPS  X10, X0
	ADDPS  X11, X1
	ADDPS  X12, X0
	ADDPS  X13, X1
	ADDPS  X14, X0
	ADDPS  X15, X1
	ADDQ   $0x000000e0, AX
	ADDQ   $0x000000e0, CX
	SUBQ   $0x00000038, DX
	JMP    unrolledLoop14

unrolledLoop8:
	CMPQ   DX, $0x00000020
	JL     unrolledLoop4
	MOVUPS (AX), X2
	MOVUPS 16(AX), X3
	MOVUPS 32(AX), X4
	MOVUPS 48(AX), X5
	MOVUPS 64(AX), X6
	MOVUPS 80(AX), X7
	MOVUPS 96(AX), X8
	MOVUPS 112(AX), X9
	MULPS  (CX), X2
	MULPS  16(CX), X3
	MULPS  32(CX), X4
	MULPS  48(CX), X5
	MULPS  64(CX), X6
	MULPS  80(CX), X7
	MULPS  96(CX), X8
	MULPS  112(CX), X9
	ADDPS  X2, X0
	ADDPS  X3, X1
	ADDPS  X4, X0
	ADDPS  X5, X1
	ADDPS  X6, X0
	ADDPS  X7, X1
	ADDPS  X8, X0
	ADDPS  X9, X1
	ADDQ   $0x00000080, AX
	ADDQ   $0x00000080, CX
	SUBQ   $0x00000020, DX
	JMP    unrolledLoop8

unrolledLoop4:
	CMPQ   DX, $0x00000010
	JL     unrolledLoop1
	MOVUPS (AX), X2
	MOVUPS 16(AX), X3
	MOVUPS 32(AX), X4
	MOVUPS 48(AX), X5
	MULPS  (CX), X2
	MULPS  16(CX), X3
	MULPS  32(CX), X4
	MULPS  48(CX), X5
	ADDPS  X2, X0
	ADDPS  X3, X1
	ADDPS  X4, X0
	ADDPS  X5, X1
	ADDQ   $0x00000040, AX
	ADDQ   $0x00000040, CX
	SUBQ   $0x00000010, DX
	JMP    unrolledLoop4

unrolledLoop1:
	CMPQ   DX, $0x00000004
	JL     tailLoop
	MOVUPS (AX), X2
	MULPS  (CX), X2
	ADDPS  X2, X0
	ADDQ   $0x00000010, AX
	ADDQ   $0x00000010, CX
	SUBQ   $0x00000004, DX
	JMP    unrolledLoop1

tailLoop:
	CMPQ  DX, $0x00000000
	JE    reduce
	MOVSS (AX), X2
	MULSS (CX), X2
	ADDSS X2, X0
	ADDQ  $0x00000004, AX
	ADDQ  $0x00000004, CX
	DECQ  DX
	JMP   tailLoop

reduce:
	ADDPS  X1, X0
	HADDPS X0, X0
	HADDPS X0, X0
	MOVSS  X0, ret+48(FP)
	RET
