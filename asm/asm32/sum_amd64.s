// Code generated by command: go run sum_asm.go -bits 32 -out ../../asm/asm32/sum_amd64.s -stubs ../../asm/asm32/sum_amd64.go -pkg asm32. DO NOT EDIT.

//go:build amd64 && gc && !noasm && !gccgo

#include "textflag.h"

// func SumAVX(x []float32) float32
// Requires: AVX, SSE
TEXT ·SumAVX(SB), NOSPLIT, $0-28
	MOVQ   x_base+0(FP), AX
	MOVQ   x_len+8(FP), CX
	VXORPS Y0, Y0, Y0
	VXORPS Y1, Y1, Y1

unrolledLoop14:
	CMPQ   CX, $0x00000070
	JL     unrolledLoop8
	VADDPS (AX), Y0, Y0
	VADDPS 32(AX), Y1, Y1
	VADDPS 64(AX), Y0, Y0
	VADDPS 96(AX), Y1, Y1
	VADDPS 128(AX), Y0, Y0
	VADDPS 160(AX), Y1, Y1
	VADDPS 192(AX), Y0, Y0
	VADDPS 224(AX), Y1, Y1
	VADDPS 256(AX), Y0, Y0
	VADDPS 288(AX), Y1, Y1
	VADDPS 320(AX), Y0, Y0
	VADDPS 352(AX), Y1, Y1
	VADDPS 384(AX), Y0, Y0
	VADDPS 416(AX), Y1, Y1
	ADDQ   $0x000001c0, AX
	SUBQ   $0x00000070, CX
	JMP    unrolledLoop14

unrolledLoop8:
	CMPQ   CX, $0x00000040
	JL     unrolledLoop4
	VADDPS (AX), Y0, Y0
	VADDPS 32(AX), Y1, Y1
	VADDPS 64(AX), Y0, Y0
	VADDPS 96(AX), Y1, Y1
	VADDPS 128(AX), Y0, Y0
	VADDPS 160(AX), Y1, Y1
	VADDPS 192(AX), Y0, Y0
	VADDPS 224(AX), Y1, Y1
	ADDQ   $0x00000100, AX
	SUBQ   $0x00000040, CX
	JMP    unrolledLoop8

unrolledLoop4:
	CMPQ   CX, $0x00000020
	JL     unrolledLoop1
	VADDPS (AX), Y0, Y0
	VADDPS 32(AX), Y1, Y1
	VADDPS 64(AX), Y0, Y0
	VADDPS 96(AX), Y1, Y1
	ADDQ   $0x00000080, AX
	SUBQ   $0x00000020, CX
	JMP    unrolledLoop4

unrolledLoop1:
	CMPQ   CX, $0x00000008
	JL     tail
	VADDPS (AX), Y0, Y0
	ADDQ   $0x00000020, AX
	SUBQ   $0x00000008, CX
	JMP    unrolledLoop1

tail:
	VXORPS X2, X2, X2

tailLoop:
	CMPQ   CX, $0x00000000
	JE     reduce
	VADDSS (AX), X2, X2
	ADDQ   $0x00000004, AX
	DECQ   CX
	JMP    tailLoop

reduce:
	VADDPS       Y0, Y1, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPS       X0, X1, X0
	VADDPS       X0, X2, X0
	VHADDPS      X0, X0, X0
	VHADDPS      X0, X0, X0
	MOVSS        X0, ret+24(FP)
	RET

// func SumSSE(x []float32) float32
// Requires: SSE, SSE3
TEXT ·SumSSE(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), AX
	MOVQ  x_len+8(FP), CX
	XORPS X0, X0
	XORPS X1, X1
	CMPQ  CX, $0x00000000
	JE    reduce
	MOVQ  AX, DX
	ANDQ  $0x0000000f, DX
	JZ    unrolledLoops
	XORQ  $0x0000000f, DX
	INCQ  DX
	SHRQ  $0x02, DX

alignmentLoop:
	MOVSS (AX), X2
	ADDSS X2, X0
	ADDQ  $0x00000004, AX
	DECQ  CX
	JZ    reduce
	DECQ  DX
	JNZ   alignmentLoop

unrolledLoops:
unrolledLoop14:
	CMPQ  CX, $0x00000038
	JL    unrolledLoop8
	ADDPS (AX), X0
	ADDPS 16(AX), X1
	ADDPS 32(AX), X0
	ADDPS 48(AX), X1
	ADDPS 64(AX), X0
	ADDPS 80(AX), X1
	ADDPS 96(AX), X0
	ADDPS 112(AX), X1
	ADDPS 128(AX), X0
	ADDPS 144(AX), X1
	ADDPS 160(AX), X0
	ADDPS 176(AX), X1
	ADDPS 192(AX), X0
	ADDPS 208(AX), X1
	ADDQ  $0x000000e0, AX
	SUBQ  $0x00000038, CX
	JMP   unrolledLoop14

unrolledLoop8:
	CMPQ  CX, $0x00000020
	JL    unrolledLoop4
	ADDPS (AX), X0
	ADDPS 16(AX), X1
	ADDPS 32(AX), X0
	ADDPS 48(AX), X1
	ADDPS 64(AX), X0
	ADDPS 80(AX), X1
	ADDPS 96(AX), X0
	ADDPS 112(AX), X1
	ADDQ  $0x00000080, AX
	SUBQ  $0x00000020, CX
	JMP   unrolledLoop8

unrolledLoop4:
	CMPQ  CX, $0x00000010
	JL    unrolledLoop1
	ADDPS (AX), X0
	ADDPS 16(AX), X1
	ADDPS 32(AX), X0
	ADDPS 48(AX), X1
	ADDQ  $0x00000040, AX
	SUBQ  $0x00000010, CX
	JMP   unrolledLoop4

unrolledLoop1:
	CMPQ  CX, $0x00000004
	JL    tailLoop
	ADDPS (AX), X0
	ADDQ  $0x00000010, AX
	SUBQ  $0x00000004, CX
	JMP   unrolledLoop1

tailLoop:
	CMPQ  CX, $0x00000000
	JE    reduce
	ADDSS (AX), X0
	ADDQ  $0x00000004, AX
	DECQ  CX
	JMP   tailLoop

reduce:
	ADDPS  X1, X0
	HADDPS X0, X0
	HADDPS X0, X0
	MOVSS  X0, ret+24(FP)
	RET
