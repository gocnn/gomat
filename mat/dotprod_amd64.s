// Code generated by command: go run dotprod_asm.go -bits 64 -out ../../mat/dotprod_amd64.s -stubs ../../mat/dotprod_amd64.go -pkg mat. DO NOT EDIT.

//go:build amd64 && gc && !noasm && !gccgo

#include "textflag.h"

// func DotProdAVX(x1 []float64, x2 []float64) float64
// Requires: AVX, FMA3, SSE2
TEXT ·DotProdAVX(SB), NOSPLIT, $0-56
	MOVQ x1_base+0(FP), AX
	MOVQ x2_base+24(FP), CX
	MOVQ x1_len+8(FP), DX
	VZEROALL

unrolledLoop14:
	CMPQ        DX, $0x00000038
	JL          unrolledLoop8
	VMOVUPD     (AX), Y2
	VMOVUPD     32(AX), Y3
	VMOVUPD     64(AX), Y4
	VMOVUPD     96(AX), Y5
	VMOVUPD     128(AX), Y6
	VMOVUPD     160(AX), Y7
	VMOVUPD     192(AX), Y8
	VMOVUPD     224(AX), Y9
	VMOVUPD     256(AX), Y10
	VMOVUPD     288(AX), Y11
	VMOVUPD     320(AX), Y12
	VMOVUPD     352(AX), Y13
	VMOVUPD     384(AX), Y14
	VMOVUPD     416(AX), Y15
	VFMADD231PD (CX), Y2, Y0
	VFMADD231PD 32(CX), Y3, Y1
	VFMADD231PD 64(CX), Y4, Y0
	VFMADD231PD 96(CX), Y5, Y1
	VFMADD231PD 128(CX), Y6, Y0
	VFMADD231PD 160(CX), Y7, Y1
	VFMADD231PD 192(CX), Y8, Y0
	VFMADD231PD 224(CX), Y9, Y1
	VFMADD231PD 256(CX), Y10, Y0
	VFMADD231PD 288(CX), Y11, Y1
	VFMADD231PD 320(CX), Y12, Y0
	VFMADD231PD 352(CX), Y13, Y1
	VFMADD231PD 384(CX), Y14, Y0
	VFMADD231PD 416(CX), Y15, Y1
	ADDQ        $0x000001c0, AX
	ADDQ        $0x000001c0, CX
	SUBQ        $0x00000038, DX
	JMP         unrolledLoop14

unrolledLoop8:
	CMPQ        DX, $0x00000020
	JL          unrolledLoop4
	VMOVUPD     (AX), Y2
	VMOVUPD     32(AX), Y3
	VMOVUPD     64(AX), Y4
	VMOVUPD     96(AX), Y5
	VMOVUPD     128(AX), Y6
	VMOVUPD     160(AX), Y7
	VMOVUPD     192(AX), Y8
	VMOVUPD     224(AX), Y9
	VFMADD231PD (CX), Y2, Y0
	VFMADD231PD 32(CX), Y3, Y1
	VFMADD231PD 64(CX), Y4, Y0
	VFMADD231PD 96(CX), Y5, Y1
	VFMADD231PD 128(CX), Y6, Y0
	VFMADD231PD 160(CX), Y7, Y1
	VFMADD231PD 192(CX), Y8, Y0
	VFMADD231PD 224(CX), Y9, Y1
	ADDQ        $0x00000100, AX
	ADDQ        $0x00000100, CX
	SUBQ        $0x00000020, DX
	JMP         unrolledLoop8

unrolledLoop4:
	CMPQ        DX, $0x00000010
	JL          unrolledLoop1
	VMOVUPD     (AX), Y2
	VMOVUPD     32(AX), Y3
	VMOVUPD     64(AX), Y4
	VMOVUPD     96(AX), Y5
	VFMADD231PD (CX), Y2, Y0
	VFMADD231PD 32(CX), Y3, Y1
	VFMADD231PD 64(CX), Y4, Y0
	VFMADD231PD 96(CX), Y5, Y1
	ADDQ        $0x00000080, AX
	ADDQ        $0x00000080, CX
	SUBQ        $0x00000010, DX
	JMP         unrolledLoop4

unrolledLoop1:
	CMPQ        DX, $0x00000004
	JL          tail
	VMOVUPD     (AX), Y2
	VFMADD231PD (CX), Y2, Y0
	ADDQ        $0x00000020, AX
	ADDQ        $0x00000020, CX
	SUBQ        $0x00000004, DX
	JMP         unrolledLoop1

tail:
	VXORPD X2, X2, X2

tailLoop:
	CMPQ        DX, $0x00000000
	JE          reduce
	VMOVSD      (AX), X3
	VFMADD231SD (CX), X3, X2
	ADDQ        $0x00000008, AX
	ADDQ        $0x00000008, CX
	DECQ        DX
	JMP         tailLoop

reduce:
	VADDPD       Y0, Y1, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPD       X0, X1, X0
	VADDPD       X0, X2, X0
	VHADDPD      X0, X0, X0
	MOVSD        X0, ret+48(FP)
	RET

// func DotProdSSE(x1 []float64, x2 []float64) float64
// Requires: SSE2, SSE3
TEXT ·DotProdSSE(SB), NOSPLIT, $0-56
	MOVQ  x1_base+0(FP), AX
	MOVQ  x2_base+24(FP), CX
	MOVQ  x1_len+8(FP), DX
	XORPD X0, X0
	XORPD X1, X1
	CMPQ  DX, $0x00000000
	JE    reduce
	MOVQ  CX, BX
	ANDQ  $0x0000000f, BX
	JZ    unrolledLoops
	MOVSD (AX), X2
	MULSD (CX), X2
	ADDSD X2, X0
	ADDQ  $0x00000008, AX
	ADDQ  $0x00000008, CX
	DECQ  DX

unrolledLoops:
unrolledLoop14:
	CMPQ   DX, $0x0000001c
	JL     unrolledLoop8
	MOVUPD (AX), X2
	MOVUPD 16(AX), X3
	MOVUPD 32(AX), X4
	MOVUPD 48(AX), X5
	MOVUPD 64(AX), X6
	MOVUPD 80(AX), X7
	MOVUPD 96(AX), X8
	MOVUPD 112(AX), X9
	MOVUPD 128(AX), X10
	MOVUPD 144(AX), X11
	MOVUPD 160(AX), X12
	MOVUPD 176(AX), X13
	MOVUPD 192(AX), X14
	MOVUPD 208(AX), X15
	MULPD  (CX), X2
	MULPD  16(CX), X3
	MULPD  32(CX), X4
	MULPD  48(CX), X5
	MULPD  64(CX), X6
	MULPD  80(CX), X7
	MULPD  96(CX), X8
	MULPD  112(CX), X9
	MULPD  128(CX), X10
	MULPD  144(CX), X11
	MULPD  160(CX), X12
	MULPD  176(CX), X13
	MULPD  192(CX), X14
	MULPD  208(CX), X15
	ADDPD  X2, X0
	ADDPD  X3, X1
	ADDPD  X4, X0
	ADDPD  X5, X1
	ADDPD  X6, X0
	ADDPD  X7, X1
	ADDPD  X8, X0
	ADDPD  X9, X1
	ADDPD  X10, X0
	ADDPD  X11, X1
	ADDPD  X12, X0
	ADDPD  X13, X1
	ADDPD  X14, X0
	ADDPD  X15, X1
	ADDQ   $0x000000e0, AX
	ADDQ   $0x000000e0, CX
	SUBQ   $0x0000001c, DX
	JMP    unrolledLoop14

unrolledLoop8:
	CMPQ   DX, $0x00000010
	JL     unrolledLoop4
	MOVUPD (AX), X2
	MOVUPD 16(AX), X3
	MOVUPD 32(AX), X4
	MOVUPD 48(AX), X5
	MOVUPD 64(AX), X6
	MOVUPD 80(AX), X7
	MOVUPD 96(AX), X8
	MOVUPD 112(AX), X9
	MULPD  (CX), X2
	MULPD  16(CX), X3
	MULPD  32(CX), X4
	MULPD  48(CX), X5
	MULPD  64(CX), X6
	MULPD  80(CX), X7
	MULPD  96(CX), X8
	MULPD  112(CX), X9
	ADDPD  X2, X0
	ADDPD  X3, X1
	ADDPD  X4, X0
	ADDPD  X5, X1
	ADDPD  X6, X0
	ADDPD  X7, X1
	ADDPD  X8, X0
	ADDPD  X9, X1
	ADDQ   $0x00000080, AX
	ADDQ   $0x00000080, CX
	SUBQ   $0x00000010, DX
	JMP    unrolledLoop8

unrolledLoop4:
	CMPQ   DX, $0x00000008
	JL     unrolledLoop1
	MOVUPD (AX), X2
	MOVUPD 16(AX), X3
	MOVUPD 32(AX), X4
	MOVUPD 48(AX), X5
	MULPD  (CX), X2
	MULPD  16(CX), X3
	MULPD  32(CX), X4
	MULPD  48(CX), X5
	ADDPD  X2, X0
	ADDPD  X3, X1
	ADDPD  X4, X0
	ADDPD  X5, X1
	ADDQ   $0x00000040, AX
	ADDQ   $0x00000040, CX
	SUBQ   $0x00000008, DX
	JMP    unrolledLoop4

unrolledLoop1:
	CMPQ   DX, $0x00000002
	JL     tailLoop
	MOVUPD (AX), X2
	MULPD  (CX), X2
	ADDPD  X2, X0
	ADDQ   $0x00000010, AX
	ADDQ   $0x00000010, CX
	SUBQ   $0x00000002, DX
	JMP    unrolledLoop1

tailLoop:
	CMPQ  DX, $0x00000000
	JE    reduce
	MOVSD (AX), X2
	MULSD (CX), X2
	ADDSD X2, X0
	ADDQ  $0x00000008, AX
	ADDQ  $0x00000008, CX
	DECQ  DX
	JMP   tailLoop

reduce:
	ADDPD  X1, X0
	HADDPD X0, X0
	MOVSD  X0, ret+48(FP)
	RET
